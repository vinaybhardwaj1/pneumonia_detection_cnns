{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook we will build 4 models to do the classification task of predicting Pnuemonia by using 5216 X-ray images to train each of our model. The descrption of models are as below:\n1. Simple Logistic Regression model with training data as flattened grayscale images.\n2. Simple LeNet-5 architechture CNN with training data as grayscale images.\n3. Modified CNN with data augmentaion encorporated to add diversity and noise in data which uses 3 channel images.\n4. Pre trained VGG-16 model along with FC layer trained on 3 channel images.\n<br><br> We evaluate each of the 4 models on the test dataset which contains 624 X-ray images."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom keras.utils import np_utils\nfrom keras import backend as K\nfrom keras.utils.np_utils import to_categorical\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import RMSprop, adam\nfrom keras import backend as K\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\n\nimport sklearn\nfrom sklearn.tree import export_graphviz\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing \nfrom sklearn.utils import shuffle\nfrom sklearn.linear_model import LogisticRegression\n\n\nfrom IPython.display import SVG\n\nimport csv\nimport os \nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport scipy as sp\nimport PIL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_name = '/kaggle/input/chest-xray-pneumonia'\n\n# assuming data is contained in a train and a validation set\ntrain_data_dir = dataset_name + '/chest_xray' + '/train/'\ntest_data_dir = dataset_name + '/chest_xray' + '/test/'\nval_data_dir = dataset_name + '/chest_xray' + '/val/'\n\n# Set up some parameters for data loading\nsample_rate = 1.0\n\n# desired dimensions of our images.\nimg_width, img_height = 162, 128\n\n# different backends (e.g. tensorflow and theano) use different orderings for image data - fix this!\nif K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)\n\ninput_shape_gray = (img_width, img_height, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing training data and transforming into required input shape (162*128). We import data in both grayscale and rgb channels."},{"metadata":{"trusted":true},"cell_type":"code","source":"training_class_folders = [i for i in os.listdir(train_data_dir) if not i.startswith('.')] # use this for full dataset\nnum_classes = len(training_class_folders)\n\n# Initialise arrays for data storage\nX_train = np.ndarray((0, input_shape[0], input_shape[1], input_shape[2]), dtype=np.float)\nX_train_gray = np.ndarray((0, input_shape[0], input_shape[1]), dtype=np.float)\ny_train= np.ndarray(0, dtype=np.str)\n    \n# Loop through the class folders\nfor i, image_cls in enumerate(training_class_folders):\n    \n    print('Processing class {}'.format(image_cls))\n    image_class_folder = train_data_dir + image_cls + \"/\"\n\n    # generate filenames from the data folder and do sampling\n    image_filenames = [image_class_folder+i for i in os.listdir(image_class_folder) if not i.startswith('.')] # use this for full dataset\n    image_filenames = random.sample(image_filenames, int(len(image_filenames)*sample_rate))\n\n    # Create a data array for image data\n    count = len(image_filenames)\n    X_data_part = np.ndarray((count, input_shape[0], input_shape[1], input_shape[2]), dtype=np.float)\n    X_data_part_gray = np.ndarray((count, input_shape[0], input_shape[1]), dtype=np.float)\n\n    # Iterate throuigh the filenames and for each one load the image, resize and normalise\n    for i, image_file in enumerate(image_filenames):\n\n        # Low the images and resize them\n        # Loading images in color\n        image = cv2.imread(image_file, cv2.IMREAD_COLOR) \n        image = cv2.resize(image, (img_height, img_width), interpolation=cv2.INTER_CUBIC)\n        image = image[:,:,[2,1,0]] # OpenCV and matplotlib use differnet channel oerderings so fix this\n        \n        # Loading images in grayscale\n        image_gray = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE) #reading images as grayscale\n        image_gray = cv2.resize(image_gray, (img_height, img_width), interpolation=cv2.INTER_CUBIC)\n        \n        # If channel order of network does not match open cv format swap it\n        if K.image_data_format() == 'channels_first':\n            image=np.swapaxes(np.swapaxes(image, 1, 2), 0, 1)\n            \n        # Add image data to data array and normalise\n        X_data_part[i] = image\n        X_data_part[i] = X_data_part[i]/255\n        \n        X_data_part_gray[i] = image_gray\n        X_data_part_gray[i] = X_data_part_gray[i]/255\n        \n        # Add label to label array\n        y_train = np.append(y_train, image_cls)\n        \n        if i%100 == 0: print('Processed {} of {} for class {} '.format(i, count, image_cls))\n\n    print('Processed {} of {} for class {} '.format(i + 1, count, image_cls))\n    \n    # Append the part to the overall data array\n    X_train = np.append(X_train, X_data_part, axis=0)\n    X_train_gray = np.append(X_train_gray, X_data_part_gray, axis=0)\n    \nprint(\"Data shape color: {}\".format(X_train.shape))\nprint(\"Data shape gray: {}\".format(X_train_gray.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Repeat the same process for test and validation data while importing."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_class_folders = [i for i in os.listdir(test_data_dir) if not i.startswith('.')] # use this for full dataset\nnum_classes = len(training_class_folders)\n\n# Initialise arrays for data storage\nX_test = np.ndarray((0, input_shape[0], input_shape[1], input_shape[2]), dtype=np.float)\nX_test_gray = np.ndarray((0, input_shape[0], input_shape[1]), dtype=np.float)\ny_test= np.ndarray(0, dtype=np.str)\n    \n# Loop through the class folders\nfor i, image_cls in enumerate(test_class_folders):\n    \n    print('Processing class {}'.format(image_cls))\n    image_class_folder = test_data_dir + image_cls + \"/\"\n\n    # generate filenames from the data folder and do sampling\n    image_filenames = [image_class_folder+i for i in os.listdir(image_class_folder) if not i.startswith('.')] # use this for full dataset\n    image_filenames = random.sample(image_filenames, int(len(image_filenames)*sample_rate))\n\n    # Create a data array for image data\n    count = len(image_filenames)\n    X_data_part = np.ndarray((count, input_shape[0], input_shape[1], input_shape[2]), dtype=np.float)\n    X_data_part_gray = np.ndarray((count, input_shape[0], input_shape[1]), dtype=np.float)\n\n    # Iterate throuigh the filenames and for each one load the image, resize and normalise\n    for i, image_file in enumerate(image_filenames):\n\n        # Low the images and resize them\n        # Loading images in color\n        image = cv2.imread(image_file, cv2.IMREAD_COLOR) \n        image = cv2.resize(image, (img_height, img_width), interpolation=cv2.INTER_CUBIC)\n        image = image[:,:,[2,1,0]] # OpenCV and matplotlib use differnet channel oerderings so fix this\n        \n        # Loading images in grayscale\n        image_gray = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE) #reading images as grayscale\n        image_gray = cv2.resize(image_gray, (img_height, img_width), interpolation=cv2.INTER_CUBIC)\n        \n        # If channel order of network does not match open cv format swap it\n        if K.image_data_format() == 'channels_first':\n            image=np.swapaxes(np.swapaxes(image, 1, 2), 0, 1)\n            \n        # Add image data to data array and normalise\n        X_data_part[i] = image\n        X_data_part[i] = X_data_part[i]/255\n        \n        X_data_part_gray[i] = image_gray\n        X_data_part_gray[i] = X_data_part_gray[i]/255\n        \n        # Add label to label array\n        y_test = np.append(y_test, image_cls)\n        \n        if i%100 == 0: print('Processed {} of {} for class {} '.format(i, count, image_cls))\n\n    print('Processed {} of {} for class {} '.format(i + 1, count, image_cls))\n    \n    # Append the part to the overall data array\n    X_test = np.append(X_test, X_data_part, axis=0)\n    X_test_gray = np.append(X_test_gray, X_data_part_gray, axis=0)\n    \nprint(\"Data shape color: {}\".format(X_test.shape))\nprint(\"Data shape gray: {}\".format(X_test_gray.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing validation data and applying transformation to resize."},{"metadata":{"trusted":true},"cell_type":"code","source":"val_class_folders = [i for i in os.listdir(val_data_dir) if not i.startswith('.')] # use this for full dataset\nnum_classes = len(training_class_folders)\n\n# Initialise arrays for data storage\nX_val = np.ndarray((0, input_shape[0], input_shape[1], input_shape[2]), dtype=np.float)\nX_val_gray = np.ndarray((0, input_shape[0], input_shape[1]), dtype=np.float)\ny_val= np.ndarray(0, dtype=np.str)\n    \n# Loop through the class folders\nfor i, image_cls in enumerate(val_class_folders):\n    \n    print('Processing class {}'.format(image_cls))\n    image_class_folder = val_data_dir + image_cls + \"/\"\n\n    # generate filenames from the data folder and do sampling\n    image_filenames = [image_class_folder+i for i in os.listdir(image_class_folder) if not i.startswith('.')] # use this for full dataset\n    image_filenames = random.sample(image_filenames, int(len(image_filenames)*sample_rate))\n\n    # Create a data array for image data\n    count = len(image_filenames)\n    X_data_part = np.ndarray((count, input_shape[0], input_shape[1], input_shape[2]), dtype=np.float)\n    X_data_part_gray = np.ndarray((count, input_shape[0], input_shape[1]), dtype=np.float)\n\n    # Iterate throuigh the filenames and for each one load the image, resize and normalise\n    for i, image_file in enumerate(image_filenames):\n\n        # Low the images and resize them\n        # Loading images in color\n        image = cv2.imread(image_file, cv2.IMREAD_COLOR) \n        image = cv2.resize(image, (img_height, img_width), interpolation=cv2.INTER_CUBIC)\n        image = image[:,:,[2,1,0]] # OpenCV and matplotlib use differnet channel oerderings so fix this\n        \n        # Loading images in grayscale\n        image_gray = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE) #reading images as grayscale\n        image_gray = cv2.resize(image_gray, (img_height, img_width), interpolation=cv2.INTER_CUBIC)\n        \n        # If channel order of network does not match open cv format swap it\n        if K.image_data_format() == 'channels_first':\n            image=np.swapaxes(np.swapaxes(image, 1, 2), 0, 1)\n            \n        # Add image data to data array and normalise\n        X_data_part[i] = image\n        X_data_part[i] = X_data_part[i]/255\n        \n        X_data_part_gray[i] = image_gray\n        X_data_part_gray[i] = X_data_part_gray[i]/255\n        \n        # Add label to label array\n        y_val = np.append(y_val, image_cls)\n        \n        if i%100 == 0: print('Processed {} of {} for class {} '.format(i, count, image_cls))\n\n    print('Processed {} of {} for class {} '.format(i + 1, count, image_cls))\n    \n    # Append the part to the overall data array\n    X_val = np.append(X_val, X_data_part, axis=0)\n    X_val_gray = np.append(X_val_gray, X_data_part_gray, axis=0)\n    \nprint(\"Data shape color: {}\".format(X_val.shape))\nprint(\"Data shape gray: {}\".format(X_val_gray.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Displaying some of the trasformed images from training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"pltsize=2\nrow_images = 5\ncol_images = 5\nplt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n\nfor i in range(row_images * col_images):\n    i_rand = random.randint(0, X_train.shape[0])\n    plt.subplot(row_images,col_images,i+1)\n    plt.axis('off')\n    plt.imshow(PIL.Image.fromarray((X_train[i_rand]*255).astype(np.uint8)))\n    plt.title((str(i_rand) + \" \" + y_train[i_rand]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also,we can output and compare our grayscale images data."},{"metadata":{"trusted":true},"cell_type":"code","source":"pltsize=2\nrow_images = 3\ncol_images = 3\nplt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n\nfor i in range(row_images * col_images):\n    i_rand = random.randint(0, X_train_gray.shape[0])\n    plt.subplot(row_images,col_images,i+1)\n    plt.axis('off')\n    plt.imshow((X_train_gray[i_rand]*255).astype('int').reshape(162,128), cmap='gray', vmin=0, vmax=255)\n    plt.title((str(i_rand) + \" \" + str(y_train[i_rand])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Creating and training a <b>logistic regression model</b> on our training data and testing immedite performance on validation data.\nWe will be using grayscale images for this.<br> Logistic regression model needs input as 2D array for training feature so we need to flatten our X_train_gray and X_val_gray for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"#initializing arrays\nX_train_lr = np.ndarray((X_train_gray.shape[0],X_train_gray[0].flatten().shape[0]), dtype=np.float)\nX_val_lr = np.ndarray((X_val_gray.shape[0],X_val_gray[0].flatten().shape[0]), dtype=np.float)\n\nfor i,j in zip(range(X_train.shape[0]),range(X_val.shape[0])):\n    X_train_lr[i] = X_train_gray[i].flatten()\n    X_val_lr[j] = X_val_gray[j].flatten()    \n    \nprint('X_train_lr shape = ',X_train_lr.shape)\nprint('X_val_lr shape = ',X_val_lr.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model = LogisticRegression().fit(X_train_lr,y_train)\ny_pred = lr_model.predict(X_train_lr)\nprint(metrics.classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing our model on validation data."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr_model.predict(X_val_lr)\nprint(metrics.classification_report(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get 50% acuracy with our flattened images from a basic Logistic regression model. But we lose the important information a 2D image can provide with the pixel structures. \n2. Next, we  create and train convolutional neural network model to recognise pneumonia in these images."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reshaping training and validation data to add channel dimention for CNN\nX_train_cnn = X_train_gray.reshape(X_train_gray.shape[0], img_width, img_height, 1)\nX_train_cnn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val_cnn = X_val_gray.reshape(X_val_gray.shape[0], img_width, img_height, 1)\nX_val_cnn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert class vectors to binary class matrices.\ny_train_encoder = sklearn.preprocessing.LabelEncoder()\ny_train_num = y_train_encoder.fit_transform(y_train)\ny_train_wide = keras.utils.to_categorical(y_train_num, num_classes)\n\ny_val_num = y_train_encoder.fit_transform(y_val)\ny_val_wide = keras.utils.to_categorical(y_val_num, num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trying out classic LeNet-5 model\n#Instantiate an empty model\nmodel = Sequential()\nmodel.add(Conv2D(6, (5, 5), input_shape=(162, 128, 1)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(16, (5, 5)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(120, (5, 5)))\nmodel.add(Activation('relu'))\n\nmodel.add(Flatten())\nmodel.add(Dense(84))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\nepochs = 20\n\nhistory = model.fit(X_train_cnn, y_train_wide,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(X_val_cnn, y_val_wide))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.plot(loss, 'blue', label='Training Loss')\nplt.plot(val_loss, 'green', label='Validation Loss')\nplt.xticks(range(0,epochs)[0::2])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a set of predictions for the validation data\npred = model.predict_classes(X_val_cnn)\n\n# Print performance details\nprint(metrics.classification_report(y_val_num, pred))\nprint(\"Confusion matrix\")\nprint(metrics.confusion_matrix(y_val_num, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Next, we will use data augmentation techniques to increase diversity in the dataset and then train a CNN on that data."},{"metadata":{"trusted":true},"cell_type":"code","source":"pltsize=2\nrow_images = 3\ncol_images = 3\n\ndatagen = ImageDataGenerator(\n    featurewise_center=False,\n    featurewise_std_normalization=False,\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1)\n\ndatagen.fit(X_train)\n\nfor idx in range(0, 4):\n    # Plot the original image\n    plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n    plt.subplot(row_images,col_images,1)\n    plt.axis('off')\n    plt.imshow(PIL.Image.fromarray((X_train[idx]*255).astype(np.uint8)))\n    plt.title(\"Original\")\n\n    for i in range(row_images * col_images - 1):\n        rand_trans = datagen.random_transform(X_train[idx])\n        plt.subplot(row_images,col_images,i+2)\n        plt.axis('off')\n        plt.imshow(PIL.Image.fromarray((rand_trans*255).astype(np.uint8)))\n        plt.title(i)\n\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_aug = Sequential()\nmodel_aug.add(Conv2D(32, (3, 3), input_shape=input_shape))\nmodel_aug.add(Activation('relu'))\nmodel_aug.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_aug.add(Conv2D(32, (3, 3)))\nmodel_aug.add(Activation('relu'))\nmodel_aug.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_aug.add(Conv2D(64, (3, 3)))\nmodel_aug.add(Activation('relu'))\nmodel_aug.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_aug.add(Flatten())\nmodel_aug.add(Dense(256, activation = 'relu'))\nmodel_aug.add(Dropout(0.5))\nmodel_aug.add(Dense(2, activation = 'softmax'))\n\nmodel_aug.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel_aug.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\nepochs = 20\n\n# Set up the callback to save the best model based on validation data\nbest_weights_filepath = './best_weights_notebook01.hdf5'\nmcp = ModelCheckpoint(best_weights_filepath, monitor=\"val_loss\",\n                      save_best_only=True, save_weights_only=False)\n\n# Create a data generator for the trianing data\ndatagen_train = ImageDataGenerator(\n    featurewise_center=False,\n    featurewise_std_normalization=False,\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1)\n\ndatagen_train.fit(X_train)\n        \nhistory_aug = model_aug.fit_generator(datagen_train.flow(X_train, y_train_wide, batch_size=batch_size),\n          steps_per_epoch=len(X_train) / batch_size,\n          validation_data=(X_val, y_val_wide),\n          epochs=epochs,\n          verbose = 1,\n          shuffle=True,\n          callbacks=[mcp])\n\n#reload best weights\nmodel_aug.load_weights(best_weights_filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history_aug.history['loss']\nval_loss = history_aug.history['val_loss']\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.plot(loss, 'blue', label='Training Loss')\nplt.plot(val_loss, 'green', label='Validation Loss')\nplt.xticks(range(0,epochs)[0::2])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a set of predictions for the validation data\npred = model_aug.predict_classes(X_val)\n\n# Print performance details\nprint(metrics.classification_report(y_val_num, pred))\nprint(\"Confusion matrix\")\nprint(metrics.confusion_matrix(y_val_num, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. For the last model we will use VGG-16 pre-trained model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the VGG16 network\nvgg16_model = keras.applications.VGG16(weights='imagenet', include_top=False, input_shape = X_train[0].shape)\ndisplay(vgg16_model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we append our old trained model with VGG-16. We will use the exact same fully connected layers from our model_aug and add to VGG-16."},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg16_last_layer = vgg16_model.output\n\n# build a classifier model to put on top of the VGG16 model\nx1 = Flatten()(vgg16_last_layer)\nx2 = Dense(256, activation='relu')(x1)\nx3 = Dropout(0.5)(x2)\nfinal_layer = Dense(2, activation = 'softmax')(x3)\n\n# Assemble the full model out of both parts\nfull_model = keras.Model(vgg16_model.input, final_layer)\n\n# moving over weights from a pre-trained smaller model specifically for our problem might help rather than random initialisation.\ntop_weights_filepath = './best_weights_notebook22.hdf5'\nold_model = keras.models.load_model(top_weights_filepath)\nfull_model.layers[-1].set_weights(old_model.layers[-1].get_weights())\n\n# set the first 17 layers (up to the last conv block)\n# to non-trainable (weights will not be updated)\nfor layer in full_model.layers[:17]:\n    layer.trainable = False\n\n# compile the model with a SGD/momentum optimizer\n# and a very slow learning rate.\nfull_model.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\nfull_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\nepochs = 20\n\n# Set up the callback to save the best model based on validation data\nbest_weights_filepath = './best_weights_notebook23.hdf5'\nmcp = ModelCheckpoint(best_weights_filepath, monitor=\"val_loss\",\n                      save_best_only=True, save_weights_only=False)\n\nhistory = full_model.fit(X_train, y_train_wide,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose = 1,\n          validation_data=(X_val, y_val_wide),\n          shuffle=True,\n          callbacks=[mcp])\n\n#reload best weights\nfull_model.load_weights(best_weights_filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.plot(loss, 'blue', label='Training Loss')\nplt.plot(val_loss, 'green', label='Validation Loss')\nplt.xticks(range(0,epochs)[0::2])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a set of predictions for the validation data\npred = np.argmax(full_model.predict(X_val),axis=1)\n\n# Print performance details\nprint(metrics.classification_report(y_val_num, pred))\nprint(\"Confusion matrix\")\nprint(metrics.confusion_matrix(y_val_num, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation"},{"metadata":{},"cell_type":"markdown","source":"We are going to test our 4 models : lr_model, model, model_aug and full_model on test data and then compare the prediction results for each of these models."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame(columns=['Model', 'Accuracy'])","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# flatten grayscale test images for LR model\nX_test_lr = np.ndarray((X_test_gray.shape[0],X_test_gray[0].flatten().shape[0]), dtype=np.float)\n\nfor i in range(X_test.shape[0]):\n    X_test_lr[i] = X_test_gray[i].flatten()\n\ny_pred = lr_model.predict(X_test_lr)\nresults.loc[0] = ['Logistic Regression',metrics.accuracy_score(y_test,y_pred)]\nprint(metrics.classification_report(y_test, y_pred))","execution_count":48,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n      NORMAL       0.38      1.00      0.55       234\n   PNEUMONIA       0.00      0.00      0.00       390\n\n    accuracy                           0.38       624\n   macro avg       0.19      0.50      0.27       624\nweighted avg       0.14      0.38      0.20       624\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"LeNet-5 based CNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reshaping training and validation data to add channel dimention for CNN\nX_test_cnn = X_test_gray.reshape(X_test_gray.shape[0], img_width, img_height, 1)\n\n# preparing y_test for CNN model comparison\ny_test_num = y_train_encoder.fit_transform(y_test)\ny_test_wide = keras.utils.to_categorical(y_test_num, num_classes)\n\n# Make a set of predictions for the validation data\ny_pred = model.predict_classes(X_test_cnn)\n\n# Print and store performance details\nresults.loc[1] = ['LeNet-5 CNN',metrics.accuracy_score(y_test_num, y_pred)]\nprint(metrics.classification_report(y_test_num, y_pred))\nprint(\"Confusion matrix\")\nprint(metrics.confusion_matrix(y_test_num, y_pred))","execution_count":49,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.97      0.40      0.56       234\n           1       0.73      0.99      0.84       390\n\n    accuracy                           0.77       624\n   macro avg       0.85      0.69      0.70       624\nweighted avg       0.82      0.77      0.74       624\n\nConfusion matrix\n[[ 93 141]\n [  3 387]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"CNN model with data augmentation added"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a set of predictions for the validation data\ny_pred = model_aug.predict_classes(X_test)\n\n# Print and store performance details\nresults.loc[2] = ['CNN + Data Augmentation',metrics.accuracy_score(y_test_num, y_pred)]\nprint(metrics.classification_report(y_test_num, y_pred))\nprint(\"Confusion matrix\")\nprint(metrics.confusion_matrix(y_test_num, y_pred))","execution_count":50,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [16, 624]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-4f5e7bc9ed4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Print and store performance details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'CNN + Data Augmentation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1969\u001b[0m     \"\"\"\n\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [16, 624]"]}]},{"metadata":{},"cell_type":"markdown","source":"VGG-16 pre trained model with fully connected layer from model_aug(previous model)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a set of predictions for the validation data\ny_pred = np.argmax(full_model.predict(X_test),axis=1)\n\n# Print and store performance details\nresults.loc[3] = ['VGG-16 + model_aug FC',metrics.accuracy_score(y_test_num, y_pred)]\nprint(metrics.classification_report(y_test_num, y_pred))\nprint(\"Confusion matrix\")\nprint(metrics.confusion_matrix(y_test_num, y_pred))","execution_count":51,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.98      0.47      0.64       234\n           1       0.76      0.99      0.86       390\n\n    accuracy                           0.80       624\n   macro avg       0.87      0.73      0.75       624\nweighted avg       0.84      0.80      0.78       624\n\nConfusion matrix\n[[111 123]\n [  2 388]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"                     Model  Accuracy\n0      Logistic Regression  0.375000\n1              LeNet-5 CNN  0.769231\n2  CNN + Data Augmentation  0.786859\n3    VGG-16 + model_aug FC  0.799679","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LeNet-5 CNN</td>\n      <td>0.769231</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CNN + Data Augmentation</td>\n      <td>0.786859</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>VGG-16 + model_aug FC</td>\n      <td>0.799679</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.set_index(['Model']).plot.bar(title='Comparison between Accuracy of 4 models - Pnuemonia Prediction')","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff37c263320>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"As expected our efforts have given us increased accuracy with the implementation of each model with the use of VGG-16 model with our own Fully Connected layer giving accuracy of 80% on the test dataset for predicting Pnuemonia."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}